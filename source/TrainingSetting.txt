SRGAN - 2017
Training on 350k ImageNet images
Testing  on Set5, Set14, BSD100, BSD300
Each mini batch has 16 Random patches of 96x96 HR patches from each trainig image

16 is depth of Generator they tested performance

SRResNet (PSNR) Training
1e-4 1e5 iterations
1e-5 1e5 iterations

SRGAN (Perceptual) Training
1e-1 1e6 Iterations
content loss scale 1/12.75, adversarial loss by 1e-3
Phi(i,j) is not defined properly in this paper



E-SRGAN
Training on DIV2K, Flickr2K and OutdoorSceneTraining (DIV2F, which merged of starting two)
Testing on Set5, Set14, BSD100, PIRM self-validation set

16 and 23 are depth of Generator they tested performance, each RRDB has 3 same structured components

1. RRDB (Residual in Residual Dense Block)
2. Relativistic Discriminator
3. VGG maps before activation (improved brightness supervision & texture recovery),
4. BatchNormalization removed, and (residual scaling(0.2) + small_initialization(0.1))
as w/o BN training becomes more difficult & Networks get stuck in local optima
5. Loss has Content, Adversarial and (also) L1 norm, which is not there in SRGAN

PSNR Training
L1 loss, with lr = 2e-4 decayed by factor of 2 after every 2e5 minibatches

Perceptual Training
loss weights = [1,5e-3,1e-2] with lr = 1e-4 halfed after [50,100,200,300]k iterations

Note: Network Interpolation experiments will not be done for now




Experimental Evaluation

There will be 2 Approaches to data feed
1. (Faster) DownSampling Full HR, then take smaller patches from Full LR Image
2. (Slower) Smaller LR patches are generated by Downsampling larger HR patches

patches_limit can work only with second approach above, while test both on training & analyzee

SRResNet PSNR Gain Results

Normal Training
DataSet Name Minimum  Maximum  Mean     Median   Standard Deviation
BSDS100      0.0241   4.5538   1.2028   1.0678   0.8341
DIV2K        0.1340   3.6828   1.3501   1.1362   0.7683
Manga109     0.4461   6.0257   3.0673   3.0346   0.9606
Set14        0.3126   3.3328   1.4685   1.2384   0.9801
Set5         0.6227   4.0698   2.1655   1.9665   1.3567
Urban100     0.3071   6.3944   1.6101   1.2864   1.0600
>>> 

Fine Tuning
DataSet Name Minimum  Maximum  Mean     Median   Standard Deviation
BSDS100      0.1496   5.6223   1.4077   1.2298   0.9561
DIV2K        0.4961   5.2995   1.6905   1.4589   1.0193
Manga109     1.2940   6.6388   3.8610   3.7790   1.1522
Set14        0.4086   3.7932   1.7533   1.5810   1.0708
Set5         0.8236   4.4487   2.6640   3.0197   1.4554
Urban100     0.3184   6.7475   1.8952   1.5763   1.1998
>>> 